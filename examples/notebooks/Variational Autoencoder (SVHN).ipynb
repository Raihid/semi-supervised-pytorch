{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "cuda = torch.cuda.is_available()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append(\"../../semi-supervised\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder\n",
    "\n",
    "The variational autoencoder (VAE) was described in its current form in [Kingma 2013](https://arxiv.org/abs/1312.6114). The model consists of an encoder/inference network $q_{\\phi}(z|x)$ and a decoder/generative network $p_{\\theta}(x|z)$. The main idea is that it is possible to both reconstruct and generate samples from from some input distribution by learning a variational distribution over the latent variable $z$.\n",
    "\n",
    "<img src=\"../images/vae.png\" width=\"300px\"/>\n",
    "\n",
    "The VAE therefore has a bottleneck structure, where the input $x$ is encoded into a latent variable $z$. New data can then be generated by feeding a latent code into the generator network - $\\widehat{x} \\sim p_{\\theta}(z|x)$. The diagram above shows the generative model (right) and how the latent variable $z$ is inferred from $x$ (left).\n",
    "\n",
    "Below we will instantiate a new variational autoencoder with this bottleneck structure consisting of a 2-layer encoder network turning an input MNIST image into a latent code: $784 \\to 256 \\to 128 \\to 32$. We also have a decoder that performs the operation in reverse: $32 \\to 128 \\to 256 \\to 784$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../semi-supervised/models/vae.py:258: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  init.xavier_normal(m.weight.data)\n"
     ]
    }
   ],
   "source": [
    "from models import VariationalAutoencoder\n",
    "from layers import GaussianSample\n",
    "model = VariationalAutoencoder([32 * 32 * 3, 300, [600, 600]]).cuda()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the middle most layer consists of a `GaussianSample` layer, in which we turn the input digit into the parameters of a Normal distribution with parameters $\\mu$ and $\\sigma$. This allows us to use the *reparametrization trick* to sample from this distribution to introduce stochasticity into the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "gaussian = GaussianSample(10, 1)\n",
    "z, mu, log_var = gaussian(Variable(torch.ones(1, 10)))\n",
    "\n",
    "print(f\"sample {float(z.data):.2f} drawn from N({float(mu.data):.2f}, {float(log_var.exp().data):.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datautils import get_mnist, get_svhn\n",
    "\n",
    "_, train, validation, std = get_svhn(location=\"./\", batch_size=256)\n",
    "# _, train, validation, mnist_mean, mnist_std = get_mnist(location=\"./\", batch_size=100, labels_per_class=10, preprocess=False)\n",
    "\n",
    "\n",
    "# We use this custom BCE function until PyTorch implements reduce=False\n",
    "def binary_cross_entropy(r, x):\n",
    "    return -torch.sum(x * torch.log(r + 1e-8) + (1 - x) * torch.log(1 - r + 1e-8), dim=-1)\n",
    "\n",
    "def mse(r, x):\n",
    "    return torch.sum(torch.pow(x - r, 2), dim=-1)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tnrange\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "for epoch in tnrange(301):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for (u, _) in train:\n",
    "        u = Variable(u)\n",
    "\n",
    "        if cuda: u = u.cuda()\n",
    "\n",
    "        reconstruction = model(u)\n",
    "        \n",
    "        likelihood = -mse(reconstruction, u)\n",
    "        elbo = likelihood - model.kl_divergence\n",
    "        \n",
    "        L = -torch.mean(elbo)\n",
    "\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += L.data.item()\n",
    "\n",
    "    m = len(train)\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch: {epoch}\\tL: {total_loss/m:.2f}\")\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        model.eval()\n",
    "        x_mu = model.sample(Variable(torch.randn(16, 300)).cuda())\n",
    "        f, axarr = plt.subplots(1, 16, figsize=(18, 12))\n",
    "        # samples = x_mu.data.cpu().numpy().reshape(-1, 28, 28)\n",
    "        \n",
    "        # mnist_means = np.tile(mnist_mean.reshape((1, -1)), (len(samples), 1))\n",
    "        # mnist_means[:, mnist_std > 0.1] = samples\n",
    "        # samples = mnist_means.reshape(-1, 28, 28)\n",
    "\n",
    "        samples = x_mu.data.view(-1, 3, 32, 32).cpu().numpy().transpose(0, 2, 3, 1)\n",
    "        for i, ax in enumerate(axarr.flat):\n",
    "            ax.imshow(samples[i], cmap=\"gray\")\n",
    "            ax.axis(\"off\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from the generative model\n",
    "\n",
    "Now that we have trained the network, we can begin to sample from it. We simply give it some random noise distributed according to the prior $p(z) = \\mathcal{N}(0, I)$ and send it through the decoder. This process generates a slew of samples that look like they come from the original distribution $p(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"vae_svhn.chckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
